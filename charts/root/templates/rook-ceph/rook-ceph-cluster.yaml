apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rook-ceph-cluster
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  destination:
    server: https://kubernetes.default.svc
    namespace: rook-ceph
  project: default
  source:
    chart: rook-ceph-cluster
    repoURL: https://charts.rook.io/release
    targetRevision: v1.13.1
    helm:
      values: |
        toolbox:
          enabled: true
        #   affinity:
        #     nodeAffinity:
        #       requiredDuringSchedulingIgnoredDuringExecution:
        #         nodeSelectorTerms:
        #         - matchExpressions:
        #           - key: name
        #             operator: In
        #             values:
        #             - node6
        configOverride: |
          [global]
          osd_pool_default_size = 2
          [mon]
          mon_data_avail_warn = 20
        monitoring:
          enabled: true
        cephClusterSpec:
          dashboard:
            enabled: true
            urlPrefix: /
            ssl: false
          storage:
            useAllNodes: false
            useAllDevices: false
            nodes:
              - name: node1
                devices:
                  - name: /dev/disk/by-id/nvme-TS512GMTE400S_I152110465
              - name: node3
                devices:
                  - name: /dev/disk/by-id/nvme-TS512GMTE400S_H908310130 
              - name: node8
                devices:
                  - name: /dev/disk/by-id/nvme-ADATA_SX8200PNP_2L2029Q18GKY
          crashCollector:
            disable: true
          placement:
            all:
              tolerations:
                - key: "node.kubernetes.io/unschedulable"
                  operator: "Exists"
                  effect: "NoSchedule"
                - key: node-role.kubernetes.io/control-plane
                  operator: "Exists"
                  effect: "NoSchedule"
            # mon:
            #   tolerations:
            #     - key: "node.kubernetes.io/unschedulable"
            #       operator: "Exists"
            #       effect: "NoSchedule"
            #     - key: node-role.kubernetes.io/control-plane
            #       operator: "Exists"
            #       effect: "NoSchedule"
        cephFileSystems: {}
        cephBlockPoolsVolumeSnapshotClass:
          enabled: false
        cephFileSystemVolumeSnapshotClass:
          enabled: false
        cephBlockPools:
          - name: ceph-blockpool
            spec:
              failureDomain: host
              replicated:
                size: 1
            storageClass:
              enabled: true
              name: ceph-block
              isDefault: true
              reclaimPolicy: Retain
              allowVolumeExpansion: true
              parameters:
                imageFormat: "2"
                csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
                csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
                csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
                csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
                csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
                csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
                csi.storage.k8s.io/fstype: ext4
        cephObjectStores: {}
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - ServerSideApply=true
